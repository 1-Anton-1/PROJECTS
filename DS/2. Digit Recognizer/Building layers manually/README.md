# 2. Digit Recognizer

## Цель:

Попрактиковаться в решении задач Computer Vision, вручную создав слои нейронной сети.

## Описание:

Создать модель машинного обучения, которая может распознавать рукописный текст, обучившись на большом датасете картинок прописных цифр MNIST.

### Исходные данные [источник](https://www.kaggle.com/competitions/digit-recognizer):
	
`train.csv` - тренировочный датафрейм, содержащий 42000 строки и 785 колонок. Каждая из 42000 ч/б картинок представлена в виде вектора в 784-мерном пространстве. Векторы в датафрейме записаны как списки, состоящие из 0 и 1 (координаты вектора), где 0 - незакрашенный пиксель картинки 28х28 (поэтому и длины списков 28х28=784), а 1 - закрашенный пиксель. Колонок 785, а не 784, потому что 1-ая колонка это маркер цифры (от 0 до 9). 

`test.csv` - тестовый датафрейм, содержащий те же колонки, что и `train`, но без колонки "label" маркеров цифр. Это данные, на которых будем проверять нашу модель компьютерного зрения. 

Описание колонок датафреймов `train` и `test`:

* label - маркер цифры, представленной в виде координат вектора в остальных колонках
* pixel0 ... pixel783 - колонки, в которых в двоичном виде представлены значения закрашенных (1) или не закрашенных пикселей (0)
		 

### В чем состоит решение задачи?



#### Краткое описание имплементации задачи:

1. Импортируем библиотеки и считываем данные из input-датафрейма (`train.csv`).
2. Ввыделим целевую колонку label в y, а остальные столбцы в датасет X
3. Выведем на экран несколько случайных цифр, чтобы визуально посмотреть как выглядят декодированные данные
4. Создадим: 
   	* функцию-нейросеть для 3 слоёв (inp-hid-out), которая позволяет получать каждый новый слой путем проходки в прямом направлении;
   	* функцию активации между слоями;
   	* функцию потерь, измеряющую среднеквадратичную разницу между получаемыми значениями (при каждом прямой проходке по нейросети) и истинными значениями (из датафрейма y)
   	* функцию обратного распространения ошибки
5. Задаем изначальные параметры нейросети (веса нейронов, смещений и т. д.) и количество проходок (эпох обучения нейросети)
6. Производим обучение нейросети методом градиентного спуска и выводим на экран значение перекрестной энтропии (величину среднекватратичного отклонения между предсказанными значениями и правильными значениями)
7. Записываем предсказания на тестовом датафрейме в csv-файл.


## Итоги
Точность предсказания нейросети обученной методом градиентного спуска оказалась равной 96,45%, что меньше точности, которую дают сверточные нейросети CNN. Однако такой подход к решению задачи позволил детально разобраться в процессе формирования и обучения нейросети методом градиентного спуска.  
