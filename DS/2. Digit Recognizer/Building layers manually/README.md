# 2. Digit Recognizer

## Цель:

Попрактиковаться в решении задач Computer Vision, вручную создав слои нейронной сети.

## Описание:

Создать модель машинного обучения, которая может распознавать рукописный текст, обучившись на большом датасете картинок прописных цифр MNIST.

### Исходные данные [источник](https://www.kaggle.com/competitions/digit-recognizer):
	
`train.csv` - тренировочный датафрейм, содержащий 42000 строки и 786 колонок. Каждая из 42000 ч/б картинок представлена в виде вектора в 786-мерном пространстве. Векторы в датафрейме записаны как списки, состоящие из 0 и 1 (координаты вектора), где 0 - незакрашенный пиксель картинки 28х28 (поэтому и длины списков 28х28=786), а 1 - закрашенный пиксель.  

`test.csv` - тестовый датафрейм, содержащий те же колонки, что и `train`, но без целевой колонки "Survived". Это данные, на которых будем проверять нашу модель (предсказывать итоговый результат). 

Описание колонок датафреймов `train` и `test`:

* PassengerId - ID пассажиров (1-891)
* Survived - целевая колонка, выжил (1) пассажир или нет (0)
* Pclass - класс пассажира от 1 до 3
* Name - полное имя пассажира
* Sex - пол 
* Age - возраст
* SibSp - это число братьев, сестер или супругов на борту у человека
* Parch - количество родителей или детей, с которыми путешествовал пассажир
* Ticket - номер билета
* Fare - тариф билета
* Cabin - номер каюты
* Embarked - порт посадки пассажира
		 

### В чем состоит решение задачи?

Просматриваем данные, делаем предобработку/аггрегацию данных (приводим их к виду, удобному для машинного обучения).
Создаем модель, обучаем модель, смотрим на предсказательную точность (predict()). Варьируем параметры модели, так чтобы точность становилась как можно выше.
Модель с оптимальными параметрами прогоняем через тестовую выборку и выводим предсказания в отдельный файл.

#### Описание имплементация задачи:

1. Импортируем библиотеки и считываем данные из input-датафрейма (`train.csv`).
2. Производим первичный анализ имеющихся данных (визуальный просмотр фичей (колонок) и выведение общих показателей таких как количество утерянных значений (NaN) в имеющихся данных).
3. Сделаем предобработку данныx:
   	* выделим целевую колонку в y, а остальные столбцы в датасет X;
   	* преобразуем строковые переменные датасета X в номинативные, применив метод pd.get_dummies(X);
   	* заменим NaN значения в колонке Age на медианное значение этой колонки;
   	* сделаем сплит (разделим на части) каждого из датасетов X и y на тренировочный и тестовый наборы в соотношении 2:1 соотвественно. Таким образом получим датасеты X_train, X_test, y_train, y_test.
4. Инициализируем экземпляр класса RandomForestClassifier ("случайный лес"), задав параметры количество эстимейторов ("число деревьев") и глубину разбиений. Тем самым создадим ML-модель.
5. Сделаем fit нашей модели (обучим на тренировочных данных (т. е. покажем нашей модели данные (значения колонок тренировочного датасета X_train ("фичи")), по которым мы ходим делать предсказания и для каждого набора данных дадим правильные ответы (колонка y_train)).
6. Посчитаем точность предсказания нашей модели на данных (сделаем скор на X_test, y_test).
7. Прочитаем данные из input-датафрейма (`test.csv`) и сделаем аналогичную предобработку данных см. п. 3 (кроме последнего подпункта)
8. Сделаем предсказание на тестовой выборке данных (сделаем предикт)
9. Запишем предсказания в `submission.csv` файл.

## Итоги
Не смотря на то, что предсказательная точноть получилась не столь велика (82% на тренировочных данных и 77,5% на тестовых данных) - удалось наглядно увидеть на практике работу ML, в частности модели из класса решающих деревьев.

