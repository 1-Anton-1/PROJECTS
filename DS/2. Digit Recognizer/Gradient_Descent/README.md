# 2. Digit Recognizer 

## Цель:

Попрактиковаться в решении задач Computer Vision, применив метод градиентного спуска.

## Описание:

Создать модель машинного обучения, которая может распознавать рукописный текст, обучившись на большом датасете картинок прописных цифр MNIST.

### Исходные данные [источник](https://www.kaggle.com/competitions/digit-recognizer):
	
`train.csv` - тренировочный датафрейм, содержащий 42000 строки и 785 колонок. Каждая из 42000 ч/б картинок представлена в виде вектора в 784-мерном пространстве. Векторы в датафрейме записаны как списки, состоящие из 0 и 1 (координаты вектора), где 0 - незакрашенный пиксель картинки 28х28 (поэтому и длины списков 28х28=784), а 1 - закрашенный пиксель. Колонок 785, а не 784, потому что 1-ая колонка это маркер цифры (от 0 до 9). 

`test.csv` - тестовый датафрейм, содержащий те же колонки, что и `train`, но без колонки "label" маркеров цифр. Это данные, на которых будем проверять нашу модель компьютерного зрения. 

Описание колонок датафреймов `train` и `test`:

* label - маркер цифры, представленной в виде координат вектора в остальных колонках
* pixel0 ... pixel783 - колонки, в которых в двоичном виде представлены значения закрашенных (1) или не закрашенных пикселей (0)
		 

### В чем состоит решение задачи?
Инициализируем трехслойную нейросеть путем функции, которая создает новый слой нейронов, основываясь на нейронах текущего слоя и учитывая веса нейронов (влияние i-го нейрона n-го слоя на j-ый нейрон слоя m) и веса смещения между соседними слоями. 
Построение слоев нейронов можно наглядно увидеть на структуре более простой сети:

<img src="https://github.com/1-Anton-1/PROJECTS/blob/main/DS/2.%20Digit%20Recognizer/Gradient_Descent/Sample_network_structures.jpg" alt="drawing" width="700"/>

Значения нейронов каждого слоя (кроме первого, т. к. первый - это входные данные Х) в нашей нейросети будем определять по формуле:

$$
\mathbf{a} = \mathrm{ReLU}\big((W)\mathbf{X} + \mathbf{b}\big),
$$

Создадим функцию потерь (которая будет говорить, о том насколько текущие значения нейронов далеки от правильных) и функцию обратного распространения ошибки (которая позволит нам вычислять обратный градиент функции ошибки, при каждом переходе между слоями (обратный градиент даст понять нейросети в какую сторону и как изменять влияние тех или иных нейронов, чтобы значения функции ошибки были минимальны)).

Зададим изначальные случайные значения весов и смещений между слоями и 1000 раз пройдем в прямом и обратном направлении. Т. е. в каждой итерации (проходке в прямом и обратном направлениях) сеть найдет средний квадрат отклонения предсказанных значений от истинных, вычислит обратный градиент функции ошибки и с заданным шагом градиентного спуска будет идти к таким значениям весов и смещений, при которых значения ошибок будет минимально.

Число в 1000 итераций было подобрано опытным путем, как оптимальное число проходок, при котором не происходит переобучения на тренировочном датафрейме.

В итоге "настроив" оптимальные значения весов смещений и нейронов получим модель, способную распознавать рукописные цифры от 0 до 9.  


#### Краткое описание имплементации задачи:

1. Импортируем библиотеки и считываем данные из input-датафрейма (`train.csv`).
2. Выделим целевую колонку label в y, а остальные столбцы в датасет X.
3. Выведем на экран несколько случайных цифр, чтобы визуально посмотреть как выглядят декодированные данные.
4. Создадим:
* функцию-нейросеть из 3 слоёв (inp-hid-out), которая позволяет получать каждый новый слой путем проходки в прямом направлении;
* функцию активации между слоями;
* функцию потерь, измеряющую среднеквадратичную разницу между получаемыми значениями (при каждой прямой проходке по нейросети) и истинными значениями (из датафрейма y);
* функцию обратного распространения ошибки.
5. Задаем изначальные параметры нейросети (веса нейронов, смещений и т. д.) и количество проходок (эпох обучения нейросети).
6. Производим обучение нейросети методом градиентного спуска и выводим на экран значение перекрестной энтропии (величину среднекватратичного отклонения между предсказанными значениями и правильными значениями).
7. Записываем предсказания на тестовом датафрейме в csv-файл.


## Итоги
Точность предсказания нейросети обученной методом градиентного спуска оказалась равной 96,45%, что меньше точности, которую дают сверточные нейросети CNN. Однако, такой подход к решению задачи позволил детально разобраться в процессе формирования и обучения нейросети методом градиентного спуска.  
